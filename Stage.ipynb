{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122c5467-665c-4c68-8f07-4244879e5ae7",
   "metadata": {},
   "source": [
    "### Uses Paper Trade - test money\n",
    "##### https://alpaca.markets/docs/api-documentation/api-v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6229c5c0-41b3-490e-aee8-a3ff32c7d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import time, datetime as dt\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "import alpha, database as db\n",
    "import alpaca_trade as alpaca\n",
    "from populate import download_data\n",
    "from populate import *\n",
    "from portfolios import *\n",
    "from history import *\n",
    "\n",
    "DataStore = db.DataStore()\n",
    "\n",
    "import copy\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cedb0ca-23bc-4ed8-930c-08be397195b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_buffer=int(1e6), batch_size=64):\n",
    "        # create state, action, next_state, reward, done tables\n",
    "        # state_dim is total # indicators or # values?; action is # actions\n",
    "        self.state = np.empty((max_buffer, state_dim))\n",
    "        self.action = np.empty((max_buffer, action_dim))\n",
    "        self.next_state = np.empty((max_buffer, state_dim))\n",
    "        self.reward = np.empty((max_buffer, 1))\n",
    "        self.done = np.empty((max_buffer, 1))\n",
    "        \n",
    "        self.max_size = max_buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.size = 0\n",
    "        self.current_memory = 0\n",
    "        self._idx = 0\n",
    "        \n",
    "    def update(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        self.state[self._idx] = state\n",
    "        self.action[self._idx] = action\n",
    "        self.next_state[self._idx] = next_state\n",
    "        self.reward[self._idx] = reward\n",
    "        self.done[self._idx] = done\n",
    "        self._idx = (self._idx + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "    def sample(self):\n",
    "        idxs = np.random.choice(self.size, self.batch_size, replace=False)\n",
    "        batch = np.vstack(self.state[idxs]), \\\n",
    "                np.vstack(self.action[idxs]), \\\n",
    "                np.vstack(self.next_state[idxs]), \\\n",
    "                np.vstack(self.reward[idxs]), \\\n",
    "                np.vstack(self.done[idxs])\n",
    "        return batch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
    "# Paper: https://arxiv.org/abs/1802.09477\n",
    "# https://github.com/sfujim/TD3/blob/master/TD3.py\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1\n",
    "\n",
    "\n",
    "class TD3(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        max_action,\n",
    "        discount=0.99,\n",
    "        tau=0.005,\n",
    "        policy_noise=0.2,\n",
    "        noise_clip=0.5,\n",
    "        policy_freq=2\n",
    "    ):\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Sample replay buffer \n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (\n",
    "                torch.randn_like(action) * self.policy_noise\n",
    "            ).clamp(-self.noise_clip, self.noise_clip)\n",
    "\n",
    "            next_action = (\n",
    "                self.actor_target(next_state) + noise\n",
    "            ).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "            # Compute actor losse\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\n",
    "            # Optimize the actor \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.actor_target = copy.deepcopy(self.actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f4be2b-a2bc-4639-a0cb-5b53ca523f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Portfolio:\n",
    "    def __init__(self, use_alpaca=True, cash=10000, positions=POPULAR):\n",
    "        self.use_alpaca = use_alpaca\n",
    "        if use_alpaca:\n",
    "            self.cash_remaining = int(alpaca.get_account()['cash'])\n",
    "            self.positions = alpaca.get_positions() # check on how returned from alpaca...\n",
    "        else:\n",
    "            self.cash_remaining = cash\n",
    "            self.positions = positions\n",
    "        print(f'Portfolio loaded - available cash: {self.cash_remaining}')\n",
    "        \n",
    "    def position(self, sym):\n",
    "        if not self.use_alpaca: return sym in self.positions\n",
    "        if sym in self.positions: # not sure format positions returned...\n",
    "            return alpaca.get_position(sym)\n",
    "        else:\n",
    "            return \"no position\"\n",
    "        \n",
    "\n",
    "class StockTraderEnvironment(gym.Env):\n",
    "    def __init__(self,\n",
    "                portfolio,\n",
    "                history, \n",
    "                short=False):\n",
    "        \"\"\"dfIndicators is a DataFrame where each column is a different indicator; short=True would allow shorting position\"\"\"\n",
    "        \n",
    "        self.portfolio = portfolio\n",
    "        self.indicators = history.indicators\n",
    "        self.prices = history.prices\n",
    "        self.symbol = history.symbol\n",
    "        num_indicators = self.indicators.shape[1]\n",
    "        assert num_indicators > 0, \"supply 1 or more indicators\"\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # set space for alpha indicators at +- infinity...?\n",
    "        low_array = np.full((num_indicators), -np.inf)\n",
    "        high_array = np.full((num_indicators), np.inf)\n",
    "        self.observation_space = spaces.Box(low=low_array, high=high_array, dtype=np.float64)\n",
    "        \n",
    "        self.nS, self.nA = self.observation_space.shape[0], self.action_space.n\n",
    "        \n",
    "        self.previous_price = 0  # didn't exist before first day, so set previous_price to 0\n",
    "        first_day = self.indicators.index[0] # starting at first day indicators exist\n",
    "        self.prices = self.prices.loc[first_day:] # rewriting prices to fit indicator list\n",
    "        \n",
    "        self.state = np.array(self.indicators.iloc[0]) # first day is inititial state\n",
    "        self.days = iter(self.prices.index.values)\n",
    "        \n",
    "        # Iterate through days, checking action/reward, etc. in step...\n",
    "        self.trades = pd.DataFrame(0, index = self.prices.index, columns = self.prices.columns)\n",
    "        self.trades_dupl = self.trades.copy(deep = True)\n",
    "        \n",
    "        # position is how much long (positive), short (negative) or holding (zero)\n",
    "        self.position = 0  # how parse?\n",
    "\n",
    "    def reset(self):\n",
    "        self.previous_price = 0\n",
    "        self.days = iter(self.prices.index.values)\n",
    "        self.state = np.array(self.indicators.iloc[0])\n",
    "        self.trades = pd.DataFrame(0, index = self.prices.index, columns = self.prices.columns)\n",
    "        self.position = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        #https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        \n",
    "        #Calculate reward here... first day = 0\n",
    "        # 0 is buy, 1 is sell, 2 is hold\n",
    "        try:\n",
    "            new_day = next(trader.days)\n",
    "            current_price = self.prices.loc[new_day]\n",
    "            self.state = self.indicators.loc[new_day]\n",
    "\n",
    "            if action == 0 and current_price > self.previous_price:\n",
    "                reward = 2\n",
    "            elif action == 0 and current_price < self.previous_price:\n",
    "                reward = -2\n",
    "            elif action == 1 and current_price < self.previous_price:\n",
    "                reward = 2\n",
    "            elif action == 1 and current_price > self.previous_price:\n",
    "                reward = -2\n",
    "            elif action == 2 and (current_price > self.previous_price or current_price < self.previous_price):\n",
    "                reward = -2\n",
    "            elif action == 2 and current_price == self.previous_price:\n",
    "                reward = 2\n",
    "            else:\n",
    "                reward = 0\n",
    "                \n",
    "            if action == 0 and self.position == 0:\n",
    "                # buying into a position, use nn probability for how much...?\n",
    "                pass\n",
    "            elif action == 0 and self.position > 0:\n",
    "                # don't overcommit to one symbol?\n",
    "                pass\n",
    "            elif action == 0 and self.position < 0:\n",
    "                # more signal to buy... keep same as last?\n",
    "                pass\n",
    "            elif action == 1 and self.position == 0:\n",
    "                # sell, with no position\n",
    "                pass\n",
    "            elif action == 1 and self.position > 0:\n",
    "                # sell, with some bought - sell all? or fraction? can't sell more than have if self.short=False\n",
    "                if not self.short:\n",
    "                    pass\n",
    "                pass\n",
    "            elif action == 1 and self.position < 0:\n",
    "                if not self.short:\n",
    "                    print(f'ERROR - short {self.symbol} with shorting disabled!')\n",
    "\n",
    "            elif action == 2:\n",
    "                # hold long or short (or no) position\n",
    "                pass\n",
    "\n",
    "            \n",
    "            self.position = portfolio.position(self.symbol)\n",
    "            self.previous_price = current_price\n",
    "            done = False\n",
    "        except StopIteration:\n",
    "            if self.trades.equals(self.trades_dupl):\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "                self.trades_dupl = self.trades.copy(deep = True)\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        #ToDo - show progression via graph?\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8df68d3f-263d-4cff-aeac-4b31beda7ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio loaded - available cash: 10000\n",
      "Setting up experiment, loading history...\n"
     ]
    }
   ],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, DataStore, td3_kwargs, portfolio, sym, dates=None, indicators='all'):\n",
    "        print('Setting up experiment, loading history...')\n",
    "        self.available_cash = portfolio.cash_remaining\n",
    "        \n",
    "        #date format for range: dates=[dt.datetime(2000,1,1), dt.datetime(2020,12,31)]\n",
    "        self.history = History(DataStore, sym, dates, indicators=indicators)\n",
    "        self.indicators = self.history.indicators\n",
    "        self.prices = self.history.prices\n",
    "        \n",
    "        self.num_days = self.prices.shape[0]\n",
    "        training = int(.75* self.num_days)\n",
    "        validation = int(.3*training)\n",
    "        test = self.num_days-training\n",
    "        \n",
    "        self.price_train = self.prices.iloc[:(training-validation)]\n",
    "        self.indicator_train = self.indicators.iloc[:(training-validation)]\n",
    "        \n",
    "        self.price_validation = self.prices.iloc[(training-validation):training]\n",
    "        self.indicator_validation = self.indicators.iloc[(training-validation):training] \n",
    "        \n",
    "        self.price_test = self.prices.iloc[training:]\n",
    "        self.indicator_test = self.indicators.iloc[training:]\n",
    "        \n",
    "        self.portfolio = portfolio\n",
    "        self.symbol = sym\n",
    "        \n",
    "        self.env = StockTraderEnvironment(self.portfolio, \n",
    "                                          self.history, \n",
    "                                          short=False) # hard-coded to prevent shorting\n",
    "        \n",
    "        self.batch_size = 64 # not parameterized...\n",
    "        self.buffer = ReplayBuffer(self.env.nS, self.env.nA, max_buffer=int(1e6), batch_size=self.batch_size)\n",
    "        \n",
    "        self.max_action = 2 # 3 actions: [0,1,2], so 2 is max\n",
    "        \n",
    "        kwargs = {\n",
    "            \"state_dim\": self.env.nS,\n",
    "            \"action_dim\": self.env.nA,\n",
    "            \"max_action\": self.max_action,\n",
    "            \"discount\": td3_kwargs['discount'],\n",
    "            \"tau\": td3_kwargs['tau'],\n",
    "            \"policy_noise\": td3_kwargs['policy_noise'],            \n",
    "            \"noise_clip\": td3_kwargs['noise_clip'],\n",
    "            \"policy_freq\": td3_kwargs['policy_freq']\n",
    "        }\n",
    "        \n",
    "        self.policy = TD3(**kwargs)\n",
    "        self.expl_noise = td3_kwargs['expl_noise']\n",
    "        \n",
    "        \n",
    "    def run(self, num_episodes, max_steps=int(1e6)):\n",
    "        \n",
    "        random_warmup = 25e3\n",
    "        total_days_run = 0\n",
    "        \n",
    "        for idx in range(num_episodes):\n",
    "            \n",
    "            state, done = self.env.reset(), False\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            for days_passed in range(self.num_days):\n",
    "                \n",
    "                steps += 1\n",
    "                \n",
    "                if total_days_run < random_warmup:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    #ToDo - what is this doing? https://github.com/sfujim/TD3/blob/master/main.py\n",
    "                    action = (self.policy.select_action(np.array(state)) + np.random.normal(0, self.max_action * self.expl_noise, size=self.env.nA)).clip(-self.max_action, self.max_action)\n",
    "            \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                experience = [state, action, next_state, reward, done]\n",
    "                self.buffer.update(experience)\n",
    "                \n",
    "                #ToDo: collect per episode, per iteration reward, total reward, etc.; portfolio value (or final portfolio value?)\n",
    "                \n",
    "                if done:\n",
    "                    print(f'Episode finished after {days_passed+1} timesteps')\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "                if days_passed >= random_warmup:\n",
    "                    policy.train(self.buffer, self.batch_size)\n",
    "                \n",
    "                if days_passed == self.num_days - 1:\n",
    "                    if num_episodes - idx - 1 == 0:\n",
    "                        print('Finished all episodes, did not converge')\n",
    "                    else:\n",
    "                        print(f'Finished all days without converging, starting from day 1 for {num_episodes - idx - 1} more episodes.')\n",
    "                        \n",
    "                total_days_run += 1\n",
    "                    \n",
    "        self.env.close() # not defined\n",
    "    \n",
    "fake_portfolio = Portfolio(use_alpaca=False)\n",
    "\n",
    "kwargs = {\n",
    "    \"discount\": 0.99,\n",
    "    \"tau\": 0.005,\n",
    "    \"policy_noise\": 0.2,            \n",
    "    \"noise_clip\": 0.5,\n",
    "    \"policy_freq\": 2,\n",
    "    \"expl_noise\": 0.1\n",
    "}\n",
    "\n",
    "exp1 = Experiment(DataStore, kwargs, portfolio=fake_portfolio, sym='AAPL',indicators=['ADX', 'CCI', 'EMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7aec21b-5d94-4529-b660-6917c79c006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History(DataStore, 'AAPL', None, indicators='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851db73e-f95e-454c-9b13-dd29d972055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockTraderEnvironment(fake_portfolio, history, short=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b211a9-8128-4a8e-b36e-59be6f0d3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = history.prices.iloc[:,:]/history.prices.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d68ee37-b183-42a3-a060-e3cdf4532d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ac371a7d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "# Load Data\n",
    "df = history.prices\n",
    "# Build App\n",
    "app = JupyterDash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H1(f'{history.symbol} price'),\n",
    "    dcc.Graph(id='graph'),\n",
    "    html.Label([\n",
    "        \"colorscale\",\n",
    "        dcc.Dropdown(\n",
    "            id='colorscale-dropdown', clearable=False,\n",
    "            value='plasma', options=[\n",
    "                {'label': c, 'value': c}\n",
    "                for c in px.colors.named_colorscales()\n",
    "            ])\n",
    "    ]),\n",
    "])\n",
    "# Define callback to update graph\n",
    "@app.callback(\n",
    "    Output('graph', 'figure'),\n",
    "    [Input(\"colorscale-dropdown\", \"value\")]\n",
    ")\n",
    "def update_figure(colorscale):\n",
    "    return px.line(\n",
    "        df, x=df.index, y=\"adjusted close\",\n",
    "        render_mode=\"webgl\"\n",
    "    )\n",
    "# Run app and display result inline in the notebook\n",
    "app.run_server(mode='inline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
