{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122c5467-665c-4c68-8f07-4244879e5ae7",
   "metadata": {},
   "source": [
    "### Uses Paper Trade - test money\n",
    "##### https://alpaca.markets/docs/api-documentation/api-v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6229c5c0-41b3-490e-aee8-a3ff32c7d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import time, datetime as dt\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "import alpha\n",
    "import alpaca\n",
    "import database as db\n",
    "from populate import download_data\n",
    "from rl_algos import TD3, ReplayBuffer, Actor, Critic\n",
    "from portfolios import Portfolio\n",
    "from history import *\n",
    "\n",
    "DataStore = db.DataStore()\n",
    "\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f4be2b-a2bc-4639-a0cb-5b53ca523f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTraderEnvironment(gym.Env):\n",
    "    def __init__(self,\n",
    "                portfolio,\n",
    "                history, \n",
    "                short=False):\n",
    "        \n",
    "        self.portfolio = portfolio\n",
    "        self.indicators = history.indicators\n",
    "        self.prices = history.prices\n",
    "        self.symbol = history.symbol\n",
    "        self.short = short\n",
    "        \n",
    "        num_indicators = self.indicators.shape[1]\n",
    "        assert num_indicators > 0, \"supply 1 or more indicators\"\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # set space for alpha indicators at +- infinity...?\n",
    "        low_array = np.full((num_indicators), -np.inf)\n",
    "        high_array = np.full((num_indicators), np.inf)\n",
    "        self.observation_space = spaces.Box(low=low_array, high=high_array, dtype=np.float64)\n",
    "        \n",
    "        self.nS, self.nA = self.observation_space.shape[0], self.action_space.n\n",
    "        \n",
    "        self.previous_price = 0  # didn't exist before first day, so set previous_price to 0\n",
    "        first_day = self.indicators.index[0] # starting at first day indicators exist\n",
    "        self.prices = self.prices.loc[first_day:] # rewriting prices to fit indicator list\n",
    "        \n",
    "        self.state = np.array(self.indicators.iloc[0]) # first day is inititial state\n",
    "        self.days = iter(self.prices.index.values)\n",
    "        \n",
    "        # Iterate through days, checking action/reward, etc. in step...\n",
    "        self.trades = pd.DataFrame(0, index = self.prices.index, columns = self.prices.columns)\n",
    "        self.trades_dupl = self.trades.copy(deep = True)\n",
    "        \n",
    "        # position is how much long (positive), short (negative) or holding (zero)\n",
    "        self.portfolio.positions.append(self.symbol)\n",
    "        self.portfolio.position_amount[self.symbol] = 0  # how parse?\n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        self.previous_price = 0\n",
    "        self.days = iter(self.prices.index.values)\n",
    "        self.state = np.array(self.indicators.iloc[0])\n",
    "        self.trades = pd.DataFrame(0, index = self.prices.index, columns = self.prices.columns)\n",
    "        self.portfolio.position_amount[self.symbol] = 0\n",
    "       \n",
    "    \n",
    "    def make_trade(self, action, current_price):\n",
    "        position = self.portfolio.position_amount[self.symbol]\n",
    "        if not self.short:\n",
    "            assert position >= 0, \"Error in logic - shorted position with shorting disabled\"\n",
    "        buysell_amount = 0\n",
    "        if action == 0 and position == 0:\n",
    "            buysell_amount = 100\n",
    "            self.portfolio.buy(self.symbol, buysell_amount, current_price)\n",
    "        elif action == 0 and position > 0:\n",
    "            buysell_amount = 50\n",
    "            self.portfolio.buy(self.symbol, buysell_amount, current_price)\n",
    "        elif action == 1 and position < 0:\n",
    "            if not self.short:\n",
    "                pass # for clarity\n",
    "            else:\n",
    "                buysell_amount = -50\n",
    "                self.portfolio.sell(self.symbol, -buysell_amount, current_price)\n",
    "        elif action == 1 and position == 0:\n",
    "            if not self.short:\n",
    "                pass\n",
    "            else:\n",
    "                buysell_amount = -100\n",
    "                self.portfolio.sell(self.symbol, -buysell_amount, current_price)\n",
    "        elif action == 1 and position > 0:\n",
    "            if not self.short:\n",
    "                buysell_amount = -position # sell off all of position if not shorting\n",
    "                self.portfolio.sell(self.symbol, -buysell_amount, current_price)\n",
    "            else:\n",
    "                buysell_amount = -position - 50 # sell off all of position if shorting and short additioanl 50\n",
    "                self.portfolio.sell(self.symbol, -buysell_amount, current_price)\n",
    "        elif action == 2:\n",
    "            pass # no action, left for clairty\n",
    "        return buysell_amount\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        #https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), action\n",
    "        \n",
    "        #Calculate reward here... first day = 0\n",
    "        # 0 is buy, 1 is sell, 2 is hold\n",
    "        try:\n",
    "            new_day = next(self.days)\n",
    "            current_price = self.prices.loc[new_day, 'adjusted close']\n",
    "            self.state = self.indicators.loc[new_day]\n",
    "\n",
    "            if action == 0 and current_price > self.previous_price:\n",
    "                reward = 2\n",
    "            elif action == 0 and current_price < self.previous_price:\n",
    "                reward = -2\n",
    "            elif action == 1 and current_price < self.previous_price:\n",
    "                reward = 2\n",
    "            elif action == 1 and current_price > self.previous_price:\n",
    "                reward = -2\n",
    "            elif action == 2 and (current_price > self.previous_price or current_price < self.previous_price):\n",
    "                reward = -2\n",
    "            elif action == 2 and current_price == self.previous_price:\n",
    "                reward = 2\n",
    "            else:\n",
    "                reward = 0\n",
    "                \n",
    "            buysell_amount = self.make_trade(action, current_price)\n",
    "            self.trades[new_day] = buysell_amount\n",
    "            \n",
    "            self.previous_price = current_price\n",
    "            done = False\n",
    "        except StopIteration:\n",
    "            if self.trades.equals(self.trades_dupl):\n",
    "                done = True\n",
    "            else:\n",
    "                self.trades_dupl = self.trades.copy(deep = True)\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        #ToDo - show progression via graph?\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df68d3f-263d-4cff-aeac-4b31beda7ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio created - available cash: 10000\n",
      "Setting up experiment, loading history...\n"
     ]
    }
   ],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, DataStore, td3_kwargs, portfolio, sym, dates=None, indicators='all', shorting_allowed=False):\n",
    "        print('Setting up experiment, loading history...')\n",
    "        self.available_cash = portfolio.cash_remaining\n",
    "        \n",
    "        #date format for range: dates=[dt.datetime(2000,1,1), dt.datetime(2020,12,31)]\n",
    "        self.history = History(DataStore, sym, dates, indicators=indicators)\n",
    "        self.indicators = self.history.indicators\n",
    "        self.prices = self.history.prices\n",
    "        \n",
    "        self.num_days = self.prices.shape[0]\n",
    "        training = int(.75* self.num_days)\n",
    "        validation = int(.3*training)\n",
    "        test = self.num_days-training\n",
    "        \n",
    "        self.price_train = self.prices.iloc[:(training-validation)]\n",
    "        self.indicator_train = self.indicators.iloc[:(training-validation)]\n",
    "        \n",
    "        self.price_validation = self.prices.iloc[(training-validation):training]\n",
    "        self.indicator_validation = self.indicators.iloc[(training-validation):training] \n",
    "        \n",
    "        self.price_test = self.prices.iloc[training:]\n",
    "        self.indicator_test = self.indicators.iloc[training:]\n",
    "        \n",
    "        self.portfolio = portfolio\n",
    "        self.symbol = sym\n",
    "        \n",
    "        self.env = StockTraderEnvironment(self.portfolio, \n",
    "                                          self.history, \n",
    "                                          short=shorting_allowed)\n",
    "        \n",
    "        self.batch_size = 64 # not parameterized...\n",
    "        self.buffer = ReplayBuffer(self.env.nS, self.env.nA, max_buffer=int(1e6), batch_size=self.batch_size)\n",
    "        \n",
    "        self.max_action = 2  # 3 actions: [0,1,2], so 2 is max\n",
    "        \n",
    "        kwargs = {\n",
    "            \"state_dim\": self.env.nS,\n",
    "            \"action_dim\": self.env.nA,\n",
    "            \"max_action\": self.max_action,\n",
    "            \"discount\": td3_kwargs['discount'],\n",
    "            \"tau\": td3_kwargs['tau'],\n",
    "            \"policy_noise\": td3_kwargs['policy_noise'],            \n",
    "            \"noise_clip\": td3_kwargs['noise_clip'],\n",
    "            \"policy_freq\": td3_kwargs['policy_freq']\n",
    "        }\n",
    "        \n",
    "        self.policy = TD3(**kwargs)\n",
    "        self.expl_noise = td3_kwargs['expl_noise']\n",
    "\n",
    "        \n",
    "    def run(self, num_episodes, max_steps=int(1e6)):\n",
    "        \n",
    "        random_warmup = 25e3\n",
    "        total_days_run = 0\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        for idx in range(num_episodes):\n",
    "            \n",
    "            state, done = self.env.reset(), False\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            for days_passed in range(self.num_days):\n",
    "                \n",
    "                steps += 1\n",
    "                \n",
    "                if total_days_run < random_warmup:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    #ToDo - what is this doing? https://github.com/sfujim/TD3/blob/master/main.py\n",
    "                    action = (self.policy.select_action(np.array(state)) + np.random.normal(0, self.max_action * self.expl_noise, size=self.env.nA)).clip(-self.max_action, self.max_action)\n",
    "                    action = np.argmax(action)\n",
    "            \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                experience = [state, action, next_state, reward, done]\n",
    "                self.buffer.update(experience)\n",
    "                \n",
    "                #ToDo: collect per episode, per iteration reward, total reward, etc.; portfolio value (or final portfolio value?)\n",
    "                self.total_reward += reward\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    print(f'Episode finished after {days_passed+1} timesteps')\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "                if days_passed >= random_warmup:\n",
    "                    self.policy.train(self.buffer, self.batch_size)\n",
    "                \n",
    "                if days_passed == self.num_days - 1:\n",
    "                    if num_episodes - idx - 1 == 0:\n",
    "                        print('Finished all episodes, did not converge')\n",
    "                    else:\n",
    "                        print(f'Finished all days without converging, starting from day 1 for {num_episodes - idx - 1} more episodes.')\n",
    "                        \n",
    "                total_days_run += 1\n",
    "                \n",
    "            print(f'episode reward {episode_reward}')\n",
    "            \n",
    "        print(f'total reward {self.total_reward}')\n",
    "                    \n",
    "        #self.env.close() # not defined\n",
    "    \n",
    "fake_portfolio = Portfolio(use_alpaca=False)\n",
    "\n",
    "kwargs = {\n",
    "    \"discount\": 0.99,\n",
    "    \"tau\": 0.005,\n",
    "    \"policy_noise\": 0.2,            \n",
    "    \"noise_clip\": 0.5,\n",
    "    \"policy_freq\": 2,\n",
    "    \"expl_noise\": 0.1\n",
    "}\n",
    "\n",
    "exp1 = Experiment(DataStore, kwargs, portfolio=fake_portfolio, sym='JPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47397463-3376-4f88-b643-573955a3154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished all days without converging, starting from day 1 for 99 more episodes.\n",
      "episode reward -3472\n",
      "Finished all days without converging, starting from day 1 for 98 more episodes.\n",
      "episode reward -3498\n",
      "Finished all days without converging, starting from day 1 for 97 more episodes.\n",
      "episode reward -3826\n",
      "Finished all days without converging, starting from day 1 for 96 more episodes.\n",
      "episode reward -3738\n",
      "Finished all days without converging, starting from day 1 for 95 more episodes.\n",
      "episode reward -2134\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fa2db193dd8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6d2c502dbd0e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_episodes, max_steps)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0;31m#ToDo - what is this doing? https://github.com/sfujim/TD3/blob/master/main.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_action\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpl_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml4t/rl_algos.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "exp1.run(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a6ac7-3038-4591-be24-607aaf0ff14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ToDo: check negation in replay buffer for done...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
