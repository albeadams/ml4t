{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122c5467-665c-4c68-8f07-4244879e5ae7",
   "metadata": {},
   "source": [
    "### Uses Paper Trade - test money\n",
    "##### https://alpaca.markets/docs/api-documentation/api-v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27f732e7-bdf0-4b1e-a01b-31ec0f4fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import time, datetime as dt\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "import alpha\n",
    "import alpaca\n",
    "import database as db\n",
    "from populate import download_data\n",
    "from rl_algos import TD3, ReplayBuffer, Actor, Critic\n",
    "from portfolios import Portfolio\n",
    "from history import *\n",
    "\n",
    "DataStore = db.DataStore()\n",
    "\n",
    "# to move to respective files...\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f4be2b-a2bc-4639-a0cb-5b53ca523f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTraderEnvironment(gym.Env):\n",
    "    def __init__(self,\n",
    "                portfolio,\n",
    "                history, \n",
    "                short=False):\n",
    "        \n",
    "        self.portfolio = portfolio\n",
    "        self.indicators = history.indicators\n",
    "        self.prices = history.prices\n",
    "        self.symbol = history.symbol\n",
    "        self.short = short\n",
    "        \n",
    "        # <<train/val/test split>>\n",
    "        num_days = self.prices.shape[0]\n",
    "        training = int(.75* num_days)\n",
    "        validation = int(.3*training)\n",
    "        test = num_days-training\n",
    "        \n",
    "        self.price_train = self.prices.iloc[:(training-validation)]\n",
    "        self.indicator_train = self.indicators.iloc[:(training-validation)]\n",
    "        \n",
    "        self.training_days = self.price_train.shape[0]\n",
    "        \n",
    "        self.price_validation = self.prices.iloc[(training-validation):training]\n",
    "        self.indicator_validation = self.indicators.iloc[(training-validation):training] \n",
    "        \n",
    "        self.price_test = self.prices.iloc[training:]\n",
    "        self.indicator_test = self.indicators.iloc[training:]\n",
    "        # <<train/val/test split>>\n",
    "        \n",
    "        \n",
    "        num_indicators = self.indicators.shape[1]\n",
    "        assert num_indicators > 0, \"supply 1 or more indicators\"\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # set space for alpha indicators at +- infinity...?\n",
    "        low_array = np.full((num_indicators), -np.inf)\n",
    "        high_array = np.full((num_indicators), np.inf)\n",
    "        self.observation_space = spaces.Box(low=low_array, high=high_array, dtype=np.float64)\n",
    "        \n",
    "        self.nS, self.nA = self.observation_space.shape[0], self.action_space.n\n",
    "        \n",
    "        self.previous_price = 0  # didn't exist before first day, so set previous_price to 0\n",
    "        first_day = self.indicator_train.index[0] # starting at first day indicators exist\n",
    "        self.prices = self.price_train.loc[first_day:] # rewriting prices to fit indicator list\n",
    "        \n",
    "        self.state = np.array(self.indicator_train.iloc[0]) # first day is inititial state\n",
    "        self.days = iter(self.price_train.index.values)\n",
    "        \n",
    "        # Iterate through days, checking action/reward, etc. in step...\n",
    "        self.trades = pd.DataFrame(0, index = self.price_train.index, columns = self.price_train.columns)\n",
    "        self.trades_dupl = self.trades.copy(deep = True)\n",
    "        \n",
    "        # position is how much long (positive), short (negative) or holding (zero)\n",
    "        self.portfolio.positions.append(self.symbol)\n",
    "        self.portfolio.position_amount[self.symbol] = 0  # how parse?\n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        self.previous_price = 0\n",
    "        self.days = iter(self.price_train.index.values)\n",
    "        self.state = np.array(self.indicator_train.iloc[0])\n",
    "        self.trades = pd.DataFrame(0, index = self.price_train.index, columns = self.price_train.columns)\n",
    "        self.portfolio.position_amount[self.symbol] = 0\n",
    "       \n",
    "    \n",
    "    def make_trade(self, action, current_price):\n",
    "        position = self.portfolio.position_amount[self.symbol]\n",
    "        if not self.short:\n",
    "            assert position >= 0, \"Error in logic - shorted position with shorting disabled\"\n",
    "        buysell_amount = 0\n",
    "        if action == 0 and position == 0:\n",
    "            buysell_amount = 100\n",
    "            self.portfolio.buy(self.symbol, buysell_amount, current_price)\n",
    "        elif action == 0 and position > 0:\n",
    "            buysell_amount = 50\n",
    "            self.portfolio.buy(self.symbol, buysell_amount, current_price)\n",
    "        elif action == 1 and position < 0:\n",
    "            if not self.short:\n",
    "                pass # for clarity\n",
    "            else:\n",
    "                buysell_amount = -50\n",
    "                self.portfolio.sell(self.symbol, -buysell_amount, current_price)\n",
    "        elif action == 1 and position == 0:\n",
    "            if not self.short:\n",
    "                pass\n",
    "            else:\n",
    "                buysell_amount = -100\n",
    "                self.portfolio.sell(self.symbol, -buysell_amount, current_price)\n",
    "        elif action == 1 and position > 0:\n",
    "            if not self.short:\n",
    "                buysell_amount = -position # sell off all of position if not shorting\n",
    "                self.portfolio.sell(self.symbol, -buysell_amount, current_price)\n",
    "            else:\n",
    "                buysell_amount = -position - 50 # sell off all of position if shorting and short additioanl 50\n",
    "                self.portfolio.sell(self.symbol, -buysell_amount, current_price)\n",
    "        elif action == 2:\n",
    "            pass # no action\n",
    "        return buysell_amount\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        #https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), action\n",
    "        \n",
    "        #Calculate reward here... first day = 0\n",
    "        # 0 is buy, 1 is sell, 2 is hold\n",
    "        try:\n",
    "            new_day = next(self.days)\n",
    "            current_price = self.price_train.loc[new_day, 'adjusted close']\n",
    "            self.state = self.indicator_train.loc[new_day]\n",
    "\n",
    "            if action == 0 and current_price > self.previous_price:\n",
    "                reward = 2\n",
    "            elif action == 0 and current_price < self.previous_price:\n",
    "                reward = -2\n",
    "            elif action == 1 and current_price < self.previous_price:\n",
    "                reward = 2\n",
    "            elif action == 1 and current_price > self.previous_price:\n",
    "                reward = -2\n",
    "            elif action == 2 and (current_price > self.previous_price or current_price < self.previous_price):\n",
    "                reward = -2 # or -1, don't puniash as much when hold and goes up/down?\n",
    "            elif action == 2 and current_price == self.previous_price:\n",
    "                reward = 2\n",
    "            else:\n",
    "                reward = 0\n",
    "                \n",
    "            buysell_amount = self.make_trade(action, current_price)\n",
    "            self.trades.loc[new_day] = buysell_amount\n",
    "            \n",
    "            info = {'current_day': new_day}\n",
    "            \n",
    "            self.previous_price = current_price\n",
    "            done = False\n",
    "        except StopIteration:\n",
    "            if self.trades.equals(self.trades_dupl):\n",
    "                done = True\n",
    "            else:\n",
    "                self.trades_dupl = self.trades.copy(deep = True)\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    \n",
    "    def validation(self):\n",
    "        # use self.price_validation and indicator_validation...\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        #ToDo - show progression via graph?\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df68d3f-263d-4cff-aeac-4b31beda7ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio created - available cash: 10000\n",
      "Setting up experiment, loading history... Ready!\n"
     ]
    }
   ],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, DataStore, td3_kwargs, portfolio, sym, dates=None, indicators='all', shorting_allowed=False):\n",
    "        print('Setting up experiment, loading history... ', end='')\n",
    "        self.available_cash = portfolio.cash_remaining\n",
    "        \n",
    "        #date format for range: dates=[dt.datetime(2000,1,1), dt.datetime(2020,12,31)]\n",
    "        self.history = History(DataStore, sym, dates, indicators=indicators)\n",
    "        self.history = self.history\n",
    "        self.indicators = self.history.indicators\n",
    "        self.prices = self.history.prices\n",
    "        \n",
    "        self.portfolio = portfolio\n",
    "        self.symbol = sym\n",
    "        \n",
    "        self.env = StockTraderEnvironment(self.portfolio, \n",
    "                                          self.history, \n",
    "                                          short=shorting_allowed)\n",
    "        \n",
    "        self.batch_size = 64 # not parameterized...\n",
    "        self.buffer = ReplayBuffer(self.env.nS, self.env.nA, max_buffer=int(1e6), batch_size=self.batch_size)\n",
    "        \n",
    "        self.max_action = 2  # 3 actions: [0,1,2], so 2 is max\n",
    "        print('Ready!')\n",
    "        \n",
    "        kwargs = {\n",
    "            \"state_dim\": self.env.nS,\n",
    "            \"action_dim\": self.env.nA,\n",
    "            \"max_action\": self.max_action,\n",
    "            \"discount\": td3_kwargs['discount'],\n",
    "            \"tau\": td3_kwargs['tau'],\n",
    "            \"policy_noise\": td3_kwargs['policy_noise']*self.max_action,            \n",
    "            \"noise_clip\": td3_kwargs['noise_clip']*self.max_action,\n",
    "            \"policy_freq\": td3_kwargs['policy_freq']\n",
    "        }\n",
    "        \n",
    "        self.policy = TD3(**kwargs)\n",
    "        self.expl_noise = td3_kwargs['expl_noise']\n",
    "\n",
    "        \n",
    "    def run(self, num_episodes, max_steps=int(1e6)):\n",
    "        \n",
    "        random_warmup = 25e3 # approx 4 years act randomly, is this enough?\n",
    "        total_days_run = 0\n",
    "        self.total_reward = 0\n",
    "        training_started = False\n",
    "        random_action = True\n",
    "        self.episode_reward = []\n",
    "        \n",
    "        for idx in range(num_episodes):\n",
    "            \n",
    "            self.env.reset()\n",
    "            state = self.env.state\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            year = None\n",
    "            \n",
    "            for days_passed in range(self.env.training_days):\n",
    "                \n",
    "                if not year:\n",
    "                    year = np.datetime64(self.history.prices.index.values[0], 'Y')\n",
    "                    end = year + np.timedelta64(11, 'M') + np.timedelta64(30, 'D')\n",
    "                \n",
    "                if total_days_run < random_warmup:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    if random_action:\n",
    "                        print('---No longer purely random---')\n",
    "                        random_action = False\n",
    "                    #ToDo - what is this doing? https://github.com/sfujim/TD3/blob/master/main.py\n",
    "                    if type(state) is np.ndarray:\n",
    "                        s = torch.from_numpy(state) # not optimal - fix original data (and for tensor so not casting to float...)\n",
    "                    else:\n",
    "                        s = torch.from_numpy(state.to_numpy())\n",
    "                    action = (self.policy.select_action(s) + np.random.normal(0, self.max_action * self.expl_noise, size=self.env.nA)).clip(-self.max_action, self.max_action)\n",
    "                    action = np.argmax(action)\n",
    "            \n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                experience = [state, action, next_state, reward, done]\n",
    "                self.buffer.update(experience)\n",
    "                \n",
    "                #ToDo: collect per episode, per iteration reward, total reward, etc.; portfolio value (or final portfolio value?)\n",
    "                self.total_reward += reward\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if end == info['current_day']:\n",
    "                    print(f'Reached end of year: {end}')\n",
    "                    year = year + 1\n",
    "                    end = year + np.timedelta64(11, 'M') + np.timedelta64(30, 'D')\n",
    "                    \n",
    "                \n",
    "                if done:\n",
    "                    print(f'Converged after {total_days_run} days')\n",
    "                    \n",
    "                    #ToDo : run on validation data when finished each epoch\n",
    "                    #    Fix: the runs only when converges - want to run at end of training days as well\n",
    "                    \n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "                if days_passed >= random_warmup:\n",
    "                    self.policy.train(self.buffer, self.batch_size)\n",
    "                    if not training_started:\n",
    "                        print('\\n----Training has begun---\\n')\n",
    "                        training_started = True\n",
    "                \n",
    "                if days_passed == self.env.training_days - 1:\n",
    "                    if num_episodes - idx - 1 == 0:\n",
    "                        print('Finished all episodes, did not converge')\n",
    "                    else:\n",
    "                        print(f'Finished episode without converging, {num_episodes - idx - 1} more episodes. ', end='')\n",
    "                        \n",
    "                total_days_run += 1\n",
    "                \n",
    "            if not done:\n",
    "                #ToDo: run validation at end of training days when not converged\n",
    "                pass\n",
    "                \n",
    "            print(f'Episode reward {episode_reward}')\n",
    "            self.episode_reward.append(episode_reward)\n",
    "            \n",
    "        print(f'total reward {self.total_reward}')\n",
    "                    \n",
    "        #self.env.close() # not defined\n",
    "    \n",
    "fake_portfolio = Portfolio(use_alpaca=False)\n",
    "\n",
    "kwargs = {\n",
    "    \"discount\": 0.99,\n",
    "    \"tau\": 0.005,\n",
    "    \"policy_noise\": 0.2,            \n",
    "    \"noise_clip\": 0.5,\n",
    "    \"policy_freq\": 2,\n",
    "    \"expl_noise\": 0.1\n",
    "}\n",
    "\n",
    "exp1 = Experiment(DataStore, kwargs, dates=[dt.datetime(2017,1,1), dt.datetime(2020,12,31)], portfolio=fake_portfolio, sym='JPM', indicators=['SMA', 'OBV', 'AD'])\n",
    "\n",
    "#ToDo: check negation in replay buffer for done...? borrow replay buffer logic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8020b885-cafb-4db1-812b-90cf93273240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio created - available cash: 10000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-54980792fb2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mfake_portfolio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPortfolio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_alpaca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataStore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JPM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SMA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OBV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0meval_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mportfolio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfake_portfolio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshorting_allowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-54980792fb2b>\u001b[0m in \u001b[0;36meval_policy\u001b[0;34m(policy, portfolio, history, shorting_allowed, seed, eval_episodes)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mavg_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml4t/rl_algos.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mcurrent_Q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_Q2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;31m# Compute critic loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_Q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_Q\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_Q2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_Q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m#critic_loss = critic_loss.double()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "\n",
    "# need this in class - to get validation data...\n",
    "# remove some logic from gym environmnet around data splitting, buy/sell, etc.\n",
    "def eval_policy(policy, portfolio, history, shorting_allowed, seed, eval_episodes=10):\n",
    "    eval_env = StockTraderEnvironment(portfolio, history, short=shorting_allowed)\n",
    "    eval_env.seed(seed + 100)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state = eval_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "fake_portfolio = Portfolio(use_alpaca=False)\n",
    "history = History(DataStore, 'JPM', dates=[dt.datetime(2017,1,1), dt.datetime(2020,12,31)], indicators=['SMA', 'OBV', 'AD'])\n",
    "eval_policy(exp1.policy, portfolio=fake_portfolio, history=history, shorting_allowed=False, seed=42, eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47397463-3376-4f88-b643-573955a3154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished all episodes, did not converge\n",
      "Episode reward -406\n",
      "total reward -406\n"
     ]
    }
   ],
   "source": [
    "exp1.run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2804d39-7cb2-4b1d-b0a6-0f4ab81e8274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(exp1.episode_reward)\n",
    "plt.ylabel('reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03cad237-2b43-46d4-8cc4-eda9d829e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_orders(df, symbol):\n",
    "    orders = df.copy(deep = True)\n",
    "    orders.columns = ['Shares']\n",
    "    orders.insert(0, 'Symbol', ''.join(symbol))\n",
    "    orders.insert(1, 'Order', 0)\n",
    "\n",
    "    orders.ix[orders['Shares'] > 0, 'Order'] = \"BUY\"\n",
    "    orders.ix[orders['Shares'] < 0, 'Order'] = \"SELL\" \n",
    "\n",
    "    # make all positive since Sell/Buy is used\n",
    "    orders['Shares'] = orders['Shares'].abs().astype(int)\n",
    "\n",
    "    return orders\n",
    "\n",
    "# not factoring in commission or impact yet...\n",
    "def compute_portvals(experiment):                                                                                      \n",
    "\n",
    "    start_val = experiment.portfolio.start_value\n",
    "    prices = experiment.env.price_train\n",
    "    orders = experiment.env.trades.copy(deep=True)\n",
    "    orders.rename(columns={'adjusted close': 'Shares'}, inplace=True)\n",
    "    orders = orders.sort_index()\n",
    "    symbol = experiment.symbol\n",
    "    \n",
    "    start_date = orders.index[0]\n",
    "    end_date = orders.index[-1]\n",
    "    dates = pd.date_range(start_date, end_date)\n",
    "    \n",
    "    prices.ffill(axis=0, inplace=True)\n",
    "    prices.bfill(axis=0, inplace=True)\n",
    "    prices['Cash'] = 1.0\n",
    "\n",
    "    trades = prices.copy(deep = True)\n",
    "    trades.loc[:,:] = 0.0\n",
    "    trades.rename(columns={'adjusted close': symbol}, inplace=True)\n",
    "\n",
    "    for date, row in orders.iterrows():\n",
    "        if row[0] < 0:\n",
    "            trades.loc[date,symbol] = trades.loc[date,symbol] - row['Shares']\n",
    "            trades.loc[date,'Cash'] = trades.loc[date,'Cash'] + 1 # track commission multiplier\n",
    "        else:\n",
    "            trades.loc[date,symbol] = trades.loc[date,symbol] + row['Shares']\n",
    "            trades.loc[date,'Cash'] = trades.loc[date,'Cash'] + 1\n",
    "\n",
    "    print(trades)\n",
    "    sys.exit()\n",
    "\n",
    "    trades['Cash'] = (prices.iloc[:,:-1].mul(trades.iloc[:,:-1]).sum(axis=1)*-1).sub(commiss['Cash'],fill_value=0).sub(df_impact['Cash'],fill_value=0)\n",
    "\n",
    "    holdings = trades.copy(deep = True)\n",
    "    \n",
    "    # each row is sum of all previous rows (excluding Cash)\n",
    "    holdings.iloc[:,:-1] = holdings.rolling(len(holdings), min_periods=1).sum()\n",
    "    holdings['Cash'][0] = holdings['Cash'][0] + start_val\n",
    "    holdings.iloc[:,-1] = holdings.iloc[:,-1].rolling(len(holdings), min_periods=1).sum()\n",
    "\n",
    "    values = holdings.copy(deep = True)\n",
    "    values.loc[:,:] = 0\n",
    "    values = prices*holdings\n",
    "\n",
    "    portvals = values.copy(deep = True)\n",
    "    portvals = values.sum(axis=1)\n",
    "    portvals = portvals.to_frame()\n",
    "    return portvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9934e12a-27b3-4d97-8a68-dcd1e94b845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              JPM  Cash\n",
      "Date                   \n",
      "2017-01-03  100.0   1.0\n",
      "2017-01-04   50.0   1.0\n",
      "2017-01-05   50.0   1.0\n",
      "2017-01-06  154.0   1.0\n",
      "2017-01-09  100.0   1.0\n",
      "...           ...   ...\n",
      "2019-02-04   50.0   1.0\n",
      "2019-02-05    0.0   1.0\n",
      "2019-02-06    0.0   1.0\n",
      "2019-02-07    0.0   1.0\n",
      "2019-02-08   50.0   1.0\n",
      "\n",
      "[529 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albert/miniconda3/envs/ml4t/lib/python3.8/site-packages/pandas/core/frame.py:4147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n",
      "<ipython-input-79-bea6a918f961>:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prices['Cash'] = 1.0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albert/miniconda3/envs/ml4t/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "compute_portvals(exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66540046-c23a-4503-aea9-be5d7c25a5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
